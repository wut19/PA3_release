{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment 3: Backpropagation and Neural Networks\n",
    "In class we have learned backpropagation algorithm and neural networks. In this section, we will build a simple neural network and use backpropagation algorithm to train the model to fit an arbitrary function. We are going to use `numpy` and `matplotlib` in Python, first let us import these packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our experiment consists of three parts:\n",
    "1. Model implementation, including linear layer, several activation functions and MSE loss.\n",
    "2. Generate training set and train our model. \n",
    "3. Analytical questions and bonus.\n",
    "\n",
    "You need to write some codes and answer some analytical questions during our experiment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-knowledge\n",
    "\n",
    "We will use some specific programming skills (i.e., [`Class`](https://www.w3schools.com/python/python_classes.asp), [`Inheritance`](https://www.w3schools.com/python/python_inheritance.asp), and [`Callable`](https://www.w3schools.com/python/ref_func_callable.asp)) in this assignment. If you are not familiar with them, you can click the corresponding hyperlink to get a reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Model Implementation\n",
    "### Basic Knowledge\n",
    "A neural network may consist of multiple layers, which includes input layer, output layer and multiple hidden layers. \n",
    "Each layer could do both forward propagation and backward propagation. Now, assume current layer is the $l$-th layer, and we denote current layer as a function $f(X, W)$, where $X$ is the input data and $W$ is the weights. \n",
    "\n",
    "- *Note that some types of layers don't have weights, which means they don't need to be updated.*\n",
    "\n",
    "In forward propagation, the layer takes input data $X$ and computes the output $Y = f(X, W)$, which will be used as input data of $(l+1)$-th layer. \n",
    "\n",
    "Backward propagation is a little more complicated. Denote the loss function of model as $\\mathcal L$. In backward propagation, we first receive a output gradient $\\frac{\\partial \\mathcal L}{\\partial f}$ which is passed from the $(L+1)$-th layer. What we want is to compute the gradients of $X$ and $W$. According to the chain rule, we have:\n",
    "$$\n",
    "\\frac{\\partial \\mathcal L}{\\partial X} = \\frac{\\partial \\mathcal L}{\\partial f}\\frac{\\partial f}{\\partial X},\n",
    "\\frac{\\partial \\mathcal L}{\\partial W} = \\frac{\\partial \\mathcal L}{\\partial f}\\frac{\\partial f}{\\partial W}.\n",
    "$$\n",
    "Further, $\\frac{\\partial \\mathcal L}{\\partial X}$ will be backward-passed to the $(l-1)$-th layer, and $\\frac{\\partial \\mathcal L}{\\partial W}$ can be used to update the weights.\n",
    "\n",
    "### Base Class\n",
    "Because all layers need to do forward propagation and backward propagation, we will define a base class first. All kinds of other layers will be derived from this class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module:\n",
    "    def __init__(self):\n",
    "        self.with_weights = False\n",
    "        pass\n",
    "    def forward(self):\n",
    "        pass\n",
    "    def backward(self):\n",
    "        pass\n",
    "    def __call__(self, *inputs, **kwargs):\n",
    "        return self.forward(*inputs, **kwargs)\n",
    "# learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we define the `__call__` method, which make the class callable. This means we can call the object of `Module` class like a function:\n",
    "```py\n",
    "model = Module()\n",
    "# equavilent to model.__call__()\n",
    "model()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "In `__call__` function, we pass the parameters to `forward` function and return the results. So we could write *pytorch*-style code and do forward propagation by call the object. \n",
    "```py\n",
    "# forward propagation\n",
    "# equavilent to model.forward(input)\n",
    "model(input)\n",
    "```\n",
    "\n",
    "The `self.with_weights` is an indicator of whether this layer's parameters will be updated in the backpropagation process. You will find more explanation in the later section about simple neural network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Layer\n",
    "A linear unit applies a linear transformation to the incoming data:\n",
    "$$\n",
    "\\mathbf y =  \\boldsymbol w^T \\boldsymbol x + b,\n",
    "$$\n",
    "where $\\boldsymbol x \\in \\mathbb R^{n}$ is the input array, $\\boldsymbol w \\in \\mathbb R^{n}$ is the weight and $b \\in \\mathbb R$ is the bias. Here, $n$ is the input dimension. \n",
    "\n",
    "![](./fig1.png) \n",
    "\n",
    "A linear layer consists of multiple linear units, the number of linear units also determines the output dimension:\n",
    "\n",
    "![](./fig2.png) \n",
    "\n",
    "We use $W \\in \\mathbb R ^{n\\times k}, \\mathbf b \\in \\mathbb R^{1 \\times k}$ to denote the weight and bias of linear layer respectively, where $W  = \\left[\\boldsymbol w^{(1)},..., \\boldsymbol w^{(k)}\\right], \\mathbf b = \\left[ b^{(1)},...,b^{(k)} \\right]$. The forward propagation of linear layer can be formulated as:\n",
    "$$\n",
    "\\mathbf y = XW + \\mathbf b,\n",
    "$$\n",
    "where $X \\in \\mathbb R^{m \\times n}$ is the input array, $y \\in \\mathbb R ^{m \\times k}$ is the output array. Here $m$ is the batch size, so we can compute a whole batch of samples together. \n",
    "\n",
    "For backward propagation, we have received an output gradient $\\frac{\\partial \\mathcal L}{\\partial Y}$, which is a $m \\times k$ matrix. And we have:\n",
    "$$\n",
    "\\frac{\\partial \\mathcal L}{\\partial W} = X^T \\frac{\\partial \\mathcal L}{\\partial Y}, \\quad \\frac{\\partial \\mathcal L}{\\partial \\mathbf b} = \\mathbf 1^T \\frac{\\partial \\mathcal L}{\\partial Y}, \\quad \\frac{\\partial \\mathcal L}{\\partial X}= \\frac{\\partial \\mathcal L}{\\partial Y} W^T,\n",
    "$$\n",
    "where $\\mathbf 1 \\in \\mathbb R^m$ is a vector with all elements equal to $1$. \n",
    "For detailed derivation, refer to [Backpropagation for a Linear Layer](http://cs231n.stanford.edu/handouts/linear-backprop.pdf).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1 (2 points). Implement the forward propagation and backward propagation of linear layer. For the forward propagation, the return value should be the value of `f(x)` at the given input `x`. For the backward propagation, we compute the gradient with respect to the input $\\frac{\\partial \\mathcal L}{\\partial X}$ as well as with respect to the layer weight $\\frac{\\partial \\mathcal L}{\\partial W}$ and $\\frac{\\partial \\mathcal L}{\\partial b}$. We will directly do the gradient descent update (e.g., $W \\leftarrow W - lr * X^T \\frac{\\partial \\mathcal L}{\\partial Y}$) on `self.W` and `self.b` based on the computed gradient;  Then return the value of $\\frac{\\partial \\mathcal L}{\\partial X}$, so that it can be passed to the previous layer. (Please note that `res` is just a variable holding the results returned from the function call. )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    def __init__(self,input_dim,output_dim):\n",
    "        super(Linear, self).__init__()\n",
    "        # initilize weights\n",
    "        self.W = np.random.randn(input_dim, output_dim) * 1e-2\n",
    "        self.b = np.zeros((1, output_dim))\n",
    "        # the linear layer's parameters should be updated\n",
    "        self.with_weights = True\n",
    "        \n",
    "    def forward(self, input_array):\n",
    "        res = np.zeros((input_array.shape[0], self.W.shape[1]))\n",
    "        \n",
    "        ## start of your code\n",
    "\n",
    "        ## end of your code\n",
    "        \n",
    "        return res\n",
    "\n",
    "    def backward(self, input_array, output_gradient, lr = 0.05):\n",
    "        res = np.zeros_like(input_array)\n",
    "        ## start of your code\n",
    "        # 1. compute new output_gradient, which will be backward-passed to previous layer\n",
    "       \n",
    "        # 2. compute the gradient and update W, b\n",
    "\n",
    "        ## end of your code\n",
    "        \n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2 (2 points). Here, we use a simple example to test our code. We set $m = n = k = 2$, $X = \\begin{bmatrix}1 & 3\\\\2 & 5\\end{bmatrix}$, $W = \\begin{bmatrix}2 & 1\\\\2 & 1\\end{bmatrix}$, $\\mathbf b = \\mathbf 0^T$, $\\frac{\\partial \\mathcal L}{\\partial Y}= \\begin{bmatrix}1 & 1\\\\1 & 1\\end{bmatrix}$. Please calculate the result for forward propagation, backward propagation, and updated parameters. We set learning rate as $0.05$ here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your Answer:*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can compare your calculation results and the following code output results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_linear = Linear(2, 2)\n",
    "input_array = np.array([[1., 3.],[2., 5.]])\n",
    "test_linear.W = np.array([[2., 1.], [2., 1.]])\n",
    "forward_res = test_linear(input_array)\n",
    "print(\"forward:\\n\", forward_res)\n",
    "# output dimension\n",
    "output_gradient =  np.ones((2,2))\n",
    "backward_res = test_linear.backward(input_array, output_gradient, lr=0.05)\n",
    "print(\"backward:\\n\", backward_res)\n",
    "print(\"W:\\n\", test_linear.W)\n",
    "print(\"b:\\n\", test_linear.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Layer\n",
    "In this section, we will implement two types of activation layers. Activation layers do not change the dimension of input array. Given an activation function, the activation layer will apply the activation function on each element of the input array and get the output that has a same shape with input array. Also, we don't need to update the parameter.\n",
    "\n",
    "![](./fig3.png)\n",
    "\n",
    "#### ReLU\n",
    "ReLU is an activation function defined as the positive part of its argument:\n",
    "$$\n",
    "\\text{R}(x) = x^+ = \\max(0, x).\n",
    "$$\n",
    "We have:\n",
    "$$\n",
    "\\text{R}'(x) = \\left\\{\\begin{matrix}1&x > 0\\\\0 & \\text{otherwise}\\end{matrix}\\right.\n",
    "$$\n",
    "So for backward propagation of ReLU layer, if the input element is negative, the corresponding element in output gradient $\\frac{\\partial \\mathcal L}{\\partial Y}$ will be set to $0$, other elements will stay the same.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3 (2 points). Implement the forward propagation and backward propagation of ReLU layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    def __init__(self):\n",
    "        super(ReLU, self).__init__()\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input_array):\n",
    "        res = np.zeros_like(input_array)\n",
    "        \n",
    "        ## start of your code\n",
    "        \n",
    "        ## end of your code\n",
    "        \n",
    "        return res\n",
    "\n",
    "    def backward(self, input_array, output_gradient):\n",
    "        res = np.zeros_like(input_array)\n",
    "        \n",
    "        ## start of your code\n",
    "\n",
    "        ## end of your code\n",
    "        \n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test ReLU layer, we set input $X = \\begin{bmatrix}2 & -2\\\\-2 & 2\\end{bmatrix}$, $\\frac{\\partial \\mathcal L}{\\partial Y}= \\begin{bmatrix}1 & 1\\\\1 & 1\\end{bmatrix}$. \n",
    "\n",
    "For forward propagation, we have a result: \n",
    "$$\n",
    "\\text{R}(X) =  \\begin{bmatrix}2 & 0\\\\0 & 2\\end{bmatrix}.\n",
    "$$\n",
    "For backward propagation, we have a result:\n",
    "$$\n",
    "\\frac{\\partial \\mathcal L}{\\partial X} =  \\begin{bmatrix}1 & 0\\\\0 & 1\\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_relu = ReLU()\n",
    "input_array = np.array([[2., -2.],[-2., 2]])\n",
    "forward_res = test_relu(input_array)\n",
    "print(\"forward:\\n\", forward_res)\n",
    "output_gradient =  np.ones((2,2))\n",
    "backward_res = test_relu.backward(input_array, output_gradient)\n",
    "print(\"backward:\\n\", backward_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sigmoid\n",
    "Sigmoid function is an activation function defined by the formula:\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}.\n",
    "$$\n",
    "We have\n",
    "$$\n",
    "\\sigma'(x) =\\sigma(x) \\left(1 - \\sigma(x)\\right)\n",
    "$$\n",
    "So for backward propagation of Sigmoid layer, we compute $\\sigma'(x)$ for each input element $x$ and multiply it with the corresponding element in output gradient $\\frac{\\partial \\mathcal L}{\\partial Y}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of the forward propagation and backward propagation of Sigmoid layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Module):\n",
    "    def __init__(self):\n",
    "        super(Sigmoid, self).__init__()\n",
    "\n",
    "    def forward(self, input_array):\n",
    "        return 1 / (1 + np.exp(-input_array))\n",
    "\n",
    "    def backward(self, input_array, output_gradient):\n",
    "        return output_gradient * self.forward(input_array) * (1 - self.forward(input_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test Sigmoid layer, we set input $X = \\begin{bmatrix}2 & -2\\\\-2 & 2\\end{bmatrix}$, $\\frac{\\partial \\mathcal L}{\\partial Y}= \\begin{bmatrix}1 & 1\\\\1 & 1\\end{bmatrix}$. \n",
    "\n",
    "For forward propagation, we have a result: \n",
    "$$\n",
    "\\sigma(X) =  \\begin{bmatrix}0.88079708 &0.26894142\\\\0.26894142 & 0.88079708\\end{bmatrix}.\n",
    "$$\n",
    "For backward propagation, we have a result:\n",
    "$$\n",
    "\\frac{\\partial \\mathcal L}{\\partial X} =  \\begin{bmatrix}0.10499359 & 0.19661193\\\\0.19661193 & 0.10499359\\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sigmoid = Sigmoid()\n",
    "input_array = np.array([[2., -1.],[-1., 2]])\n",
    "forward_res = test_sigmoid(input_array)\n",
    "print(\"forward:\\n\", forward_res)\n",
    "output_gradient =  np.ones((2,2))\n",
    "backward_res = test_sigmoid.backward(input_array, output_gradient)\n",
    "print(\"backward:\\n\", backward_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MSE Loss\n",
    "To train our model, we need to define our loss function. Because we want to do a regression problem, we will use mean squared error (MSE) as our loss function:\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{m}\\sum_{i=1}^m\\left(\\hat y^{(i)} - y^{(i)}\\right)^2.\n",
    "$$\n",
    "Loss can be considered as a special layer. In forward propagation, it takes the predicted value and true value as input and returns the error. For backward propagation, it will compute the gradients of output layer. Note that we don't need a output gradient here, because it's the final layer in training process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of the forward propagation and backward propagation of MSE Loss layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss(Module):\n",
    "    def __init__(self):\n",
    "        super(MSELoss, self).__init__()   \n",
    "        \n",
    "    def forward(self, predicted_y, y):\n",
    "        return np.mean((predicted_y - y) ** 2)\n",
    "\n",
    "    def backward(self, predicted_y, y):\n",
    "        return (predicted_y - y) * 2 / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test MSE loss, we set predicted value $\\hat{ \\mathbf y} = \\begin{bmatrix}1 \\\\1\\end{bmatrix}$ and true value $\\mathbf y = \\begin{bmatrix}1.5 \\\\1.5\\end{bmatrix}$. \n",
    "\n",
    "For forward propagation, we have a result: \n",
    "$$\n",
    "\\text{MSE}  =  0.25.\n",
    "$$\n",
    "For backward propagation, we have a result:\n",
    "$$\n",
    "\\frac{\\partial \\mathcal L}{\\partial \\hat{ \\mathbf y}} =  \\begin{bmatrix}-0.5 \\\\ -0.5\\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mse = MSELoss()\n",
    "pred_y = np.array([[1], [1]])\n",
    "true_y = np.array([[1.5], [1.5]])\n",
    "forward_res = test_mse(pred_y, true_y)\n",
    "print(\"forward:\\n\", forward_res)\n",
    "backward_res = test_mse.backward(pred_y, true_y)\n",
    "print(\"backward:\\n\", backward_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Neural Network\n",
    "By now, we can actually design a neural network with all these layers and loss function. Here, we define a new class to connect all the components.  You just need to list all the layers you want to use by order, and input the list and loss function when construct the model. \n",
    "\n",
    "In `forward` function, we will start from the first layer, which takes the original features as input, and the output will be the input data for the next layer. We use a for-loop for forward propagation. All the intermediate input array will be stored for later backward propagation. \n",
    "\n",
    "In `backward` function, we firstly compute the loss and output gradient generated from loss function. Then, this output gradient will be passed to output layer. Start from the output layer, we update the parameters and backward-pass the output gradient to previous layers through a reversed for-loop. \n",
    "\n",
    "Therefore, by calling `forward` and `backward`, our model will automatically do the propagation and update parameters. However, you need to make sure your design is reasonable. *If the dimensions between different layers don't match, there will be errors.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(Module):\n",
    "    def __init__(self, layers, loss, lr = 0.005):\n",
    "        super(SimpleNN, self).__init__()    \n",
    "        self.layers = layers\n",
    "        self.loss = loss\n",
    "        self.inputs = [None for _ in range(len(self.layers))]\n",
    "        self.output = None\n",
    "        self.loss_value = 1e5\n",
    "        self.lr = lr\n",
    "        pass\n",
    "\n",
    "    def forward(self, input_array):\n",
    "        current_input = input_array\n",
    "        for i in range(len(self.layers)):\n",
    "            self.inputs[i] = current_input\n",
    "            current_input = self.layers[i](current_input)\n",
    "        self.output = current_input\n",
    "        return self.output\n",
    "        \n",
    "    def backward(self, y):\n",
    "        if self.inputs[-1] is None:\n",
    "            print(\"call forward first.\")\n",
    "            return\n",
    "        self.loss_value = self.loss(self.output, y)\n",
    "        output_gradient = self.loss.backward(self.output, y)\n",
    "        for i in range(len(self.layers)-1, -1, -1):\n",
    "            # here we check wheather a layer should be updated in the backpropagation by judging its `.with_weights` value.\n",
    "            # whether we update the weight is determined by whether the layer has trainable parameters. \n",
    "            # for instance, a layer such as x^2 or ||x||^2 does not have a parameter, so we set `.with_weight`` to false.\n",
    "            if not isinstance(self.inputs[i], np.ndarray):\n",
    "                self.inputs[i]=self.inputs[i].numpy()\n",
    "            if self.layers[i].with_weights:\n",
    "                output_gradient = self.layers[i].backward(self.inputs[i], output_gradient, self.lr)\n",
    "            else:\n",
    "                output_gradient = self.layers[i].backward(self.inputs[i], output_gradient)\n",
    "        self.output = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "Here, we will design a simple neural network. The structure of our model is shown in following figure:\n",
    "\n",
    "![](./fig4.png)\n",
    "Our model contains:\n",
    "- L0, Input layer (shape: $N \\times 1$), where $N$ is the batch size.\n",
    "- L1, Linear layer (shape: $1 \\times 80$)\n",
    "- L2, ReLU layer\n",
    "- L3, Linear layer (shape: $80 \\times 80$)\n",
    "- L4, ReLU layer\n",
    "- L5, Output layer (linear layer, shape: $80 \\times 1$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [\n",
    "    # input layer is input data\n",
    "    # L1\n",
    "    Linear(1, 80),\n",
    "    # L2\n",
    "    ReLU(),  # Sigmoid(),# \n",
    "    # L3\n",
    "    Linear(80, 80),\n",
    "    # L4\n",
    "    ReLU(),  # Sigmoid(),# \n",
    "    # L5\n",
    "    Linear(80, 1)\n",
    "]\n",
    "loss = MSELoss()\n",
    "model = SimpleNN(layers, loss, lr=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use this model to approximate the sine function. Our training set $\\left\\{x^{(i)}, y^{(i)}: i=1,2, \\ldots, m\\right\\}$ is generated by \n",
    "$$\n",
    "y^{(i)}=\\sin x^{(i)}, \\quad \\forall i.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-np.pi, np.pi, 100).reshape(-1, 1) \n",
    "y = np.sin(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, train our model and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "max_iteration = 20000\n",
    "plt.figure(figsize=(15, 8))\n",
    "pred_y = model(x)\n",
    "plt.plot(x, pred_y, label=\"init\")\n",
    "plt.plot(x, y, label=\"true\")\n",
    "while epoch < max_iteration and model.loss_value > 1e-4:\n",
    "    pred_y = model(x)\n",
    "    model.backward(y)\n",
    "    if epoch % 500 == 0:\n",
    "        print(r'epoch {}/{}, loss: {}'.format(epoch, max_iteration, model.loss_value))\n",
    "    epoch += 1\n",
    "plt.plot(x, pred_y, label=\"pred\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytical Questions\n",
    "\n",
    "\n",
    "**Please make sure you understand how the SimpleNN class works with the layer classes you implemented.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4 (2 points). In the training, we use Sigmoid as the activation function. Compare the training speed and results of using Sigmoid and ReLU. Then, change the sample size of the dataset to 1000 (you can also try other sizes), and compare the training speed and results. Please report the running time and loss. Describe the differences and their potential factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer:*   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q5 (2 points). Try to remove all the activation layers. Compare the results and analyze the role of activation layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hint on L2 Regularization\n",
    "\n",
    "Some students are confused about where to put the L2 regularization of linear layers. You probably think about adding the regularization term when defining the MSELoss, however we cannot get the weights of linear layers in loss layer.\n",
    "\n",
    "Actually, L2 regularization can be put in the backward propagation of linear layer when updating the weights. With L2 regularization term, our final loss function becomes:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(X, W) = MSE + \\lambda \\sum_{i=1}^{|L|} ||W_i||_2^2,\n",
    "$$\n",
    "where $W_i$ is the weight of i-th layers. So the gradient of each layer's weight becomes:\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_i} = \\frac{\\partial MSE}{\\partial W_i} + \\lambda \\frac{\\partial ||W_i||_2^2}{\\partial W_i}.\n",
    "$$\n",
    "The first term is computed through backward propagation, which is what we have done. Also notice that the second term is only related to the weight of the i-th layer: $\\frac{\\partial ||W_i||_2^2}{\\partial W_i}=2W_i$. There is no need to do backward propagation for regularization. So it can be put in `backward` function when updating the weights.\n",
    "\n",
    "#### Q6 (1 point). Add L2 regularization for linear layer. What difference is made by adding L2 regularization in terms of model training and model accuracy?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## L2 regularization\n",
    "\n",
    "class Linear(Module):\n",
    "    def __init__(self,input_dim,output_dim):\n",
    "        super(Linear, self).__init__()\n",
    "        # initilize weights\n",
    "        self.W = np.random.randn(input_dim, output_dim) * 1e-2\n",
    "        self.b = np.zeros((1, output_dim))\n",
    "        # the linear layer's parameters should be updated\n",
    "        self.with_weights = True\n",
    "        \n",
    "    def forward(self, input_array):\n",
    "        res = np.zeros((input_array.shape[0], self.W.shape[1]))\n",
    "        \n",
    "        ## start of your code\n",
    "        \n",
    "        ## end of your code\n",
    "        \n",
    "        return res\n",
    "\n",
    "    def backward(self, input_array, output_gradient, lr = 0.05):\n",
    "        res = np.zeros_like(input_array)\n",
    "        ## start of your code\n",
    "        \n",
    "        \n",
    "        ## end of your code\n",
    "        \n",
    "        return res\n",
    "\n",
    "layers = [\n",
    "    # input layer is input data\n",
    "    # L1\n",
    "    Linear(1, 80),\n",
    "    # L2\n",
    "    ReLU(),# \n",
    "    # L3\n",
    "    Linear(80, 80),\n",
    "    # L4\n",
    "    ReLU(),# \n",
    "    # L5\n",
    "    Linear(80, 1)\n",
    "]\n",
    "loss = MSELoss()\n",
    "model = SimpleNN(layers, loss, lr=0.05)\n",
    "\n",
    "x = np.linspace(-np.pi, np.pi, 100).reshape(-1, 1) \n",
    "y = np.sin(x)\n",
    "\n",
    "epoch = 0\n",
    "max_iteration = 20000\n",
    "plt.figure(figsize=(15, 8))\n",
    "pred_y = model(x)\n",
    "plt.plot(x, pred_y, label=\"init\")\n",
    "plt.plot(x, y, label=\"true\")\n",
    "time0 = time.time()\n",
    "while epoch < max_iteration and model.loss_value > 1e-4:\n",
    "    pred_y = model(x)\n",
    "    model.backward(y)\n",
    "    time1 = time.time()\n",
    "    if epoch % 500 == 0:\n",
    "        print(r'epoch {}/{}, loss: {}, spent time: {}'.format(epoch, max_iteration, model.loss_value, time1-time0))\n",
    "    epoch += 1\n",
    "plt.plot(x, pred_y, label=\"pred\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next task is to use this model to classfy different numbers in MNIST. First, we need to load MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device to CPU\n",
    "device = \"cpu\"\n",
    "\n",
    "# Define data transformation: Convert images to tensors and normalize\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[0.5], std=[0.5])  # Normalization\n",
    "])\n",
    "\n",
    "# Load MNIST dataset\n",
    "path = './data/'\n",
    "TRAIN_DATA = torchvision.datasets.MNIST(path, train=True, transform=transform, download=False)\n",
    "VAL_DATA = torchvision.datasets.MNIST(path, train=False, transform=transform, download=False)\n",
    "\n",
    "# Function to prepare data, sampling a specified number of samples for each digit\n",
    "def prepare_data(data, num_samples):\n",
    "    selected_indices = []\n",
    "    label_counts = {i: 0 for i in range(10)}  # Track sample count for each digit\n",
    "\n",
    "    # Iterate through the dataset to select the required number of samples\n",
    "    for i in range(len(data)):\n",
    "        _, lbl = data[i]\n",
    "        if label_counts[lbl] < num_samples:\n",
    "            selected_indices.append(i)\n",
    "            label_counts[lbl] += 1\n",
    "\n",
    "        # Exit the loop once all digit samples are collected\n",
    "        if all(count >= num_samples for count in label_counts.values()):\n",
    "            break\n",
    "\n",
    "    # Create a subset based on the selected indices\n",
    "    final_data = torch.utils.data.Subset(data, selected_indices)\n",
    "\n",
    "    # Create DataLoader\n",
    "    data_loader = torch.utils.data.DataLoader(final_data, batch_size=1, shuffle=False)\n",
    "    \n",
    "    return final_data, data_loader\n",
    "\n",
    "# Set the number of samples per digit\n",
    "num_samples_train = 50  # samples for each digit in training\n",
    "num_samples_val = 20     # samples for each digit in validation\n",
    "\n",
    "# Prepare training and validation data\n",
    "train_data, train_data_loader = prepare_data(TRAIN_DATA, num_samples_train)\n",
    "val_data, val_data_loader = prepare_data(VAL_DATA, num_samples_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-class Cross-entropy Loss\n",
    "The multi-class cross-entropy loss function is used to measure the difference between the predicted probability distribution of a model and the true class distribution. It can be formulated as:\n",
    "$$\n",
    "L = -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{c=1}^{C} y_{i,c} \\log(p_{i,c})\n",
    "$$\n",
    "\n",
    "$$\n",
    "p_{i,c} = \\frac{e^{z_{i,c}}}{\\sum_{j=1}^{C} e^{z_{i,j}}}\n",
    "$$\n",
    "where\n",
    "\\begin{align*}\n",
    "L & : \\text{The multi-class cross-entropy loss.} \\\\\n",
    "N & : \\text{The number of samples in the dataset.} \\\\\n",
    "C & : \\text{The number of classes.} \\\\\n",
    "y_{i,c} & : \\text{The true label for sample } i \\text{ and class } c. \\text{ It is } 1 \\text{ if sample } i \\text{ belongs to class } c, \\text{ and } 0 \\text{ otherwise.} \\\\\n",
    "p_{i,c} & : \\text{The predicted probability that sample } i \\text{ belongs to class } c. \\\\\n",
    "z_{i,c} & : \\text{The raw score (logit)for sample} i \\text{ and class } c.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q7 (2 points): Implement the forward propagation and backward propagation of Cross-entropy Loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss(Module):\n",
    "    def __init__(self):\n",
    "        super(CrossEntropyLoss, self).__init__()   \n",
    "        \n",
    "    def forward(self, predicted_y, y):\n",
    "        N = predicted_y.shape[0]\n",
    "        \n",
    "        # start of your code\n",
    "        # Compute probabilities using softmax\n",
    "        exp_logits = \n",
    "        probabilities = \n",
    "        \n",
    "        # Calculate cross-entropy loss\n",
    "        loss = \n",
    "        # end of your code\n",
    "        return loss.numpy()\n",
    "\n",
    "    def backward(self, predicted_y, y):\n",
    "        N = predicted_y.shape[0]\n",
    "\n",
    "        # start of your code\n",
    "        grad = \n",
    "        # end of your code\n",
    "        return grad.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [\n",
    "    # input layer is input data\n",
    "    # L1\n",
    "    Linear(784, 80),\n",
    "    # L2\n",
    "    ReLU(),  # Sigmoid(),# \n",
    "    # L3\n",
    "    Linear(80, 80),\n",
    "    # L4\n",
    "    ReLU(),  # Sigmoid(),# \n",
    "    # L5\n",
    "    Linear(80, 10)\n",
    "]\n",
    "loss = CrossEntropyLoss()\n",
    "model = SimpleNN(layers, loss, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "max_iteration = 200\n",
    "while epoch < max_iteration:\n",
    "    loss = 0\n",
    "    for i,(img,lbl) in enumerate(train_data_loader):\n",
    "        img=img.view(-1,784)\n",
    "        lbl=torch.nn.functional.one_hot(lbl, num_classes=10).float()\n",
    "        pred_y = model(img)\n",
    "        _, predicted = torch.max(pred_y, 1)\n",
    "        model.backward(lbl)\n",
    "        loss += model.loss_value\n",
    "    if epoch % 5 == 0:\n",
    "        print(r'epoch {}/{}, loss: {}'.format(epoch, max_iteration, loss / len(train_data_loader)))\n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, train our model and print the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct=0\n",
    "total=len(val_data_loader)\n",
    "for img, lbl in val_data_loader:\n",
    "    img = img.view(-1, 784) \n",
    "    pred_y = model(img)\n",
    "    _, predicted = torch.max(pred_y, 1)\n",
    "\n",
    "    correct += (predicted.item() == lbl.item())  \n",
    "accuracy = correct / total  \n",
    "print(f'Validation Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q8 (1 points). In the training, hyperparameters like learning rate will greatly affect the performance. By adjusting the learning rate, you can observe how it influences convergence and overall accuracy. Additionally, please plot a graph to visualize the results, showing the relationship between learning rates and model performance. This will help identify the optimal learning rate for different tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your Answer:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background for Q9 & Q10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional layers are commonly used in modern neural networks. In the following questions, you will be asked to implement a simplified 1-d convolutional neural network.\n",
    "\n",
    "A convolution operation between a 1-d signal $s$ of length $T$ with a 1-d kernel  K of length $T'$ can be understood as sliding the kernel $K$ from left to right of signal $s$ and perform dot product at each position. \n",
    "\n",
    "$$s * k[\\tau] = \\sum_{t'=0}^{T'-1} s[\\tau+t'] K[\\tau]  $$\n",
    "\n",
    "For example, let's say we have $S= [a~ b~ c~ d~ e~ f]$ and $K=[x~y~z]$, then the convolved output will be $[ax+by+cz~bx+cy+dz~cx+dy+ez~dx+ey+fz]$. \n",
    "\n",
    "\n",
    "![](./conv1d.png)\n",
    "Note that such a convolution operation alters the temporal length of the input signals. Oftentimes people want the temporal lengths remain the same between the input and the output signals. Then extra padding values should be concatenated to the input signal before convolution. e.g. To make $f$ have the same temporal length as $s$, we can pad $s$ to be $s_p=[0~a~b~c~d~e~f~0]$ and conduct convolution with $K$ thereafter. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "When we have multiple   1-d signals, we can use define batch convolution using matrices.\n",
    "\n",
    "Given signals $S\\in \\mathbb{R}^{B \\times T}$, where $B$ is batch size and $T$ is the temporal length, together with a kernel matrix $K\\in \\mathbb{R}^{T'}$, where $T'$ is the temporal length of the convolution kernel, we compute the feature map $F=S*K$ as: \n",
    "\n",
    "$$F[i, \\tau]=\\sum_{t'=0}^{T'-1} S[i, \\tau+t'] K[t']$$\n",
    "\n",
    "where $i\\in [0, 1, \\dots, B-1]$ and $\\tau\\in [0, 1,\\dots, T-T']$. \n",
    "\n",
    "\n",
    " In a 1D convolutional layer, we are given (zero-padded) input signal matrix $X$ , a learnable kernel  $W$ and a learnable bias term $b$. And the layer output is $y =  X * W + b$.   *(For simplicity, we assume channel size and number of filters are both 1.)*\n",
    "\n",
    "After the convolution operation, a scalar bias term is added to each temporal position of the output signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q9 (0.5 point,Bonus). Derive the gradient of the 1d-convolutional layer. You should represent $\\frac{\\partial l}{\\partial w}$, $\\frac{\\partial l}{\\partial b}$ and $\\frac{\\partial l}{\\partial x}$ with $\\frac{\\partial l}{\\partial y}$, w, b, and x, where $l$ is the MSE loss without regularization, y is the output of the convolution operation without padding, w is the kernel ($K$ in the previous example), b is the bias term and x is the input to the convolutional layer. Hint: you can first calculate $\\frac{\\partial y_{bi}}{\\partial w_{k}}$, and then apply the chain rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your Answer:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q10 (1 points,Bonus). Implement the forward and backward propagation of a 1d convolutional layer, after which train a simple 1d convolutional network. You can use loops, although we suggest using vectorized operations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1d(Module):\n",
    "    def __init__(self, kernel_size, pad_length):\n",
    "        # kernel_size: the length between the middle point to the leftmost / rightmost point\n",
    "        #   In other words, if you set kernel_size=a, then the kernel has 1 + 2*a parameters.\n",
    "        #   For our example where the kernel is x|y|z, the kernel_size is 1.\n",
    "        # pad_length: after padding, the input signal will be padded by zero on the both sides of temporal dimension, \n",
    "        #   thereby having shape B, T+2*pad_length\n",
    "        #   For our example with padding, pad_length=1\n",
    "        super(Conv1d, self).__init__()\n",
    "        # initilize weights\n",
    "        self.W = np.zeros((1, 1 + 2*kernel_size))\n",
    "        self.b = np.zeros((1, 1))\n",
    "        self.kernel_size = kernel_size\n",
    "        self.pad_length = pad_length\n",
    "        # the layer's parameters should be updated\n",
    "        self.with_weights = True\n",
    "        \n",
    "    def forward(self, input_array):\n",
    "        ## start of your code\n",
    "        \n",
    "        ## end of your code\n",
    "        \n",
    "        return res\n",
    "\n",
    "    def backward(self, input_array, output_gradient, lr = 0.05):\n",
    "        # res = np.zeros_like(input_array)\n",
    "        \n",
    "        ## start of your code\n",
    "        # NOTE: pay attention to the padded ones when calculating gradients\n",
    "        # 1. compute new output_gradient, which will be backward-passed to previous layer\n",
    "        \n",
    "        # 2. compute the gradient and update W\n",
    "        \n",
    "        ## end of your code\n",
    "        \n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "Here, we will design a simple neural network. You are free to adjust the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [\n",
    "    # input layer is input data\n",
    "    Conv1d(2, 3),\n",
    "    ReLU(),\n",
    "    Conv1d(1, 0),\n",
    "    Linear(16, 64),\n",
    "    ReLU(),\n",
    "    Linear(64, 80),\n",
    "    ReLU(),\n",
    "    Linear(80, 1)\n",
    "]\n",
    "loss = MSELoss()\n",
    "model = SimpleNN(layers, loss, lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import pi\n",
    "x = np.random.randn(2, 16)\n",
    "y = np.cos(np.random.randn(2, 1)*pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "max_iteration = 20000\n",
    "plt.figure(figsize=(15, 8))\n",
    "pred_y = model(x)\n",
    "plt.plot(x, pred_y, label=\"init\")\n",
    "plt.plot(x, y, label=\"true\")\n",
    "while epoch < max_iteration and model.loss_value > 1e-4:\n",
    "    pred_y = model(x)\n",
    "    model.backward(y)\n",
    "    if epoch % 500 == 0:\n",
    "        print(r'epoch {}/{}, loss: {}'.format(epoch, max_iteration, model.loss_value))\n",
    "    epoch += 1\n",
    "plt.plot(x, pred_y, label=\"pred\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "3d597f4c481aa0f25dceb95d2a0067e73c0966dcbd003d741d821a7208527ecf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
